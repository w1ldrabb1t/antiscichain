
p.37
> "Hormesis a world coined by pharmacologists is when *a small dose of a harmful substance is actual beneficial for the organism, acting as medicine*."

p.44 p.45 p.46
> "Anti-fragile automatic responses are just a form of redundancy. Nature likes to overinsure itself."

> "Over-compensation is a form of redundancy. A system that overcompensates is necessarily in overshooting mode, building extra capacity and strength in anticipation of a worse outcome and in response to information about the possibility of a hazard."

> "Risk management professionals look in the past for the so called worst-case scenario and use it to estimate future risks-this method is called stress testing. 
> They take the worst historical recession, the worst war, the worst historical move in interest rates, or the worst point in unemployment as an exact estimate for the worst future outcome. But they never noticed the following inconsistency: *this so-called worst case event, when it happened, exceeded the worst case at the time! Well, nature prepares for what has not happened before, assuming worst harm is possible*."

Pag 49
> "Information is anti-fragile it feeds more on attempts to harm it then it does on efforts to promote it."

Pag 57
> "*Errors and their consequences are information*; for small children pain is the only risk management information, as their logical faculties are not very developed. For complex systems are well all about information. This is what we will call __causal opacity__: __it is hard to see the arrow from cause to consequence__, making much of conventional methods of analysis, in addition to standard logic, inapplicable. 
> "The predictability of specific events is low, and it is such opacity that makes it low. Not only that, but because of non-linearities, one needs higher visibility than with regular systems - instead what we have is opacity."

p.65
> "The antifragility of some comes at the expense of the fragility of others."
- in other words, like Taleb says, "what kills me makes others stronger"

> "In a system, the sacrifices of some units - fragile units, that is, or people - are often necessary for the well-being of other units or the whole."

p.66
> "Sadly, the benefits of errors are often conferred on others, the collective - as if individuals were designed to make errors for the greater good, not heir own"
- it's not guaranteed that others will benefit from one's mistakes.
- learning from other's mistakes comes at a cost - the cost of paying attention, understanding what happened and why it happened, and then, somehow, be able to recall these lessons to avoid making the same mistakes.

p.67
> "Unlike with hormesis, the unit does not get stronger in response to stress; it dies. But it accomplishes a transfer of benefits; other units survive - and those units that survive have attributes that imporve the collective of units, leading to modifications commonly assigned the vague term 'evolution'."
> "The most interesting aspect of evolution is that it only works because of its __antifragility__; it's in love with stressors, randomness, uncertainty, and disorder - while individual organisms are relatevely fragile, the gene pool takes advantage of schocks to enhance its fitness."

p.68
> "By leeting organisms go one lifespan at a time, with modificiations between successive generations, nature does not need to predict fugure contitions (...) Every random event will bring its own antidote in the form of ecological variation."

> "Systems subject to randomness - and unpredictability - build a mechanism beyond the robust to opportunistically reinvent themselves each generation, with a continuous change of population and species."

p.71
> "When you are fragile, you __depend__ on things following the exact planned course, with as little deviation as possible - for deviations are more harmful than helpful. This is why the fragile needs to be very predictive in its approach, and, conversely, predictive systems __cause__ fragility. When you __want__ deviations, and yuou do't care about the possible dispersion of outcomes that the future can bring, since most will be helpful, you are antifragile."

> "using error as a source of information. If every trial provides you with information about __what does not work__, you start zooming in on a solution - so every attempt becomes more valuable, more like an expense than an error."
This the key of antifragile learning!

Antifragile learning aims not to reduce the number of mistakes or errors - *it aims to learn from them*, even if the new found knowledge is about what does not work.

Antifragile learning is not concerned about being certain of how things work - *it's primary focus is on self-correcting as fast as possible towards an increasingly more accurate knowledge base*. 

Antifragile learning embraces the fact that *knowledge is fragile*. Having said that, the process of improving one's knowledge can be made to be antifragile, if we systematically improve our understanding of how a system works by both via negativa (what does not work) and via positiva (what seems to work) - while leaving the door open for refutation to come in.

Antifragile learning creates a stark contrast with how the education system works and, notoriously, how modern scientific research, especially if said research is heavily sponsored by profit oriented companies and organizations looking to find evidence that their products work by doing no harm (looking for evidence of absense!). 

The education system and scientific research is very linear (and fragile) because their primary focus is summarized by:
1. the less mistakes you make, the better you are
2. learning is about recalling answers to known problems
3. the more evidence you get that something works, the more certain you are that it does work




p.418
> "The tragedy is that it is very hard to get funding to replicate - and reject - existing studies. And even if there were money for it, it would be hard to find takers: trying to replicate studies will not make anyone a hero."

p.417
> "There is an optionality on the part of the researcher, no different from that of a banker. The researcher gets the upside, truth gets the downside. The researcher's free option is in *his ability to pick what statistics can confirm his belief - or show a good result - and ditch the rest*. He has the option to stop once he has the __right__ result."
- This is why combining scientific research with capitalism is a dangerous idea.
- Capitalist ventures will sponsor scientific research that confirms that their theory, system or product works.
- They will stop once they found __proof__ that it works (and ignore signs that it doesn't).
- This might be tolerable if the consequences of putting their theory, system or product in practice, only impacts themselves (#skininthegame)
- The problem becomes when the consequences can harm others (ruin scale) and how much harm is going to be inflicted on them (ruin impact)

  
