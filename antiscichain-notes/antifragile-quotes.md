
p.37
> "Hormesis a world coined by pharmacologists is when *a small dose of a harmful substance is actual beneficial for the organism, acting as medicine*."

p.44 p.45 p.46
> "Anti-fragile automatic responses are just a form of redundancy. Nature likes to overinsure itself."

> "Over-compensation is a form of redundancy. A system that overcompensates is necessarily in overshooting mode, building extra capacity and strength in anticipation of a worse outcome and in response to information about the possibility of a hazard."

> "Risk management professionals look in the past for the so called worst-case scenario and use it to estimate future risks-this method is called stress testing. 
> They take the worst historical recession, the worst war, the worst historical move in interest rates, or the worst point in unemployment as an exact estimate for the worst future outcome. But they never noticed the following inconsistency: *this so-called worst case event, when it happened, exceeded the worst case at the time! Well, nature prepares for what has not happened before, assuming worst harm is possible*."

Pag 49
> "Information is anti-fragile it feeds more on attempts to harm it then it does on efforts to promote it."

Pag 57
> "*Errors and their consequences are information*; for small children pain is the only risk management information, as their logical faculties are not very developed. For complex systems are well all about information. This is what we will call __causal opacity__: __it is hard to see the arrow from cause to consequence__, making much of conventional methods of analysis, in addition to standard logic, inapplicable. 
> "The predictability of specific events is low, and it is such opacity that makes it low. Not only that, but because of non-linearities, one needs higher visibility than with regular systems - instead what we have is opacity."

p.65
> "The antifragility of some comes at the expense of the fragility of others."
- in other words, like Taleb says, "what kills me makes others stronger"

> "In a system, the sacrifices of some units - fragile units, that is, or people - are often necessary for the well-being of other units or the whole."

p.66
> "Sadly, the benefits of errors are often conferred on others, the collective - as if individuals were designed to make errors for the greater good, not heir own"
- it's not guaranteed that others will benefit from one's mistakes.
- learning from other's mistakes comes at a cost - the cost of paying attention, understanding what happened and why it happened, and then, somehow, be able to recall these lessons to avoid making the same mistakes.

p.67
> "Unlike with hormesis, the unit does not get stronger in response to stress; it dies. But it accomplishes a transfer of benefits; other units survive - and those units that survive have attributes that imporve the collective of units, leading to modifications commonly assigned the vague term 'evolution'."
> "The most interesting aspect of evolution is that it only works because of its __antifragility__; it's in love with stressors, randomness, uncertainty, and disorder - while individual organisms are relatevely fragile, the gene pool takes advantage of schocks to enhance its fitness."

p.68
> "By leeting organisms go one lifespan at a time, with modificiations between successive generations, nature does not need to predict fugure contitions (...) Every random event will bring its own antidote in the form of ecological variation."

> "Systems subject to randomness - and unpredictability - build a mechanism beyond the robust to opportunistically reinvent themselves each generation, with a continuous change of population and species."

p.71
> "When you are fragile, you __depend__ on things following the exact planned course, with as little deviation as possible - for deviations are more harmful than helpful. This is why the fragile needs to be very predictive in its approach, and, conversely, predictive systems __cause__ fragility. When you __want__ deviations, and yuou do't care about the possible dispersion of outcomes that the future can bring, since most will be helpful, you are antifragile."

> "using error as a source of information. If every trial provides you with information about __what does not work__, you start zooming in on a solution - so every attempt becomes more valuable, more like an expense than an error."
This the key to [antifragile learning](antifragile-learning.md)!

p.72
> "we are talking of partial, not general, mistakes, small, not severe and terminal ones. This creates *a separation between good and bad systems*. Good systems such as airlines ar eset up to have small errors, independent from each other - or, in effect, negatively correlated to each other, since mistakes lower the odds of future mistakes. If every plane crash makes the next one less likely, every bank crash  makes the next one more likely. *We need to eliminate the second type of error - the one that produces contagion.*"

p.76
> "What did not kill me __did not__ make me stronger, but spared me __because__ I am stronger than others; but it killed others and the *average population is now stronger because the weak are gone*."

p.85
> "*Nature loves small errors* (without which genetic variations are impossible), humans don't - hence when you rely on human judgement you are at the mercy of a mental bias that disfavors antifragility. So, alas, we humans are afraid of the second type of variability and navively fragilize systems - or prevent their antifragility - by protecting them. In other words, *this avoidance of small mistakes makes the large ones more severe*."

p.87
> "This great variety of people and their wallets are there, in Switzerland, for its shelter, safety and stability. But all these refugees don't notice the obvious: *the most stable country in the world __does not have a government__*. And it is not stable in spite of not having a government; it is stable __because__ it does have one. (...) This bottom-up dorm of dictatorship provides protection against the romanticism of utopias, since no big ideas can be generated in such an intellectual atmosphere."
This is a very important point to support the reason why my platform does now have a governing body (eg. DAO).
The platform and its value should stand on its own legs.
Having no goverment in my case means no possibility of bias or corruption.
The trust must come from knowing how the system works (it's open source) and the fact that the technology will execute the code that was programmed with exactly as it should.

p.418
> "The tragedy is that it is very hard to get funding to replicate - and reject - existing studies. And even if there were money for it, it would be hard to find takers: trying to replicate studies will not make anyone a hero."

p.417
> "There is an optionality on the part of the researcher, no different from that of a banker. The researcher gets the upside, truth gets the downside. The researcher's free option is in *his ability to pick what statistics can confirm his belief - or show a good result - and ditch the rest*. He has the option to stop once he has the __right__ result."
- This is why combining scientific research with capitalism is a dangerous idea.
- Capitalist ventures will sponsor scientific research that confirms that their theory, system or product works.
- They will stop once they found __proof__ that it works (and ignore signs that it doesn't).
- This might be tolerable if the consequences of putting their theory, system or product in practice, only impacts themselves (#skininthegame)
- The problem becomes when the consequences can harm others (ruin scale) and how much harm is going to be inflicted on them (ruin impact)


